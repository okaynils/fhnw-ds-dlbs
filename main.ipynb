{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Challenge: Deep Learning for Images and Signals\n",
    "- Name: Nils Fahrni\n",
    "- Submission Date: t.b.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the performance of a U-Net semantic segmentation model differ between scenes of city streets and non-city streets in the BDD100K dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"dlbs\"\n"
     ]
    }
   ],
   "source": [
    "#%env WANDB_SILENT=True\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"dlbs\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Berkeley Deep Drive Dataset: https://arxiv.org/abs/1805.04687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_PATH = os.path.join('data', 'bdd100k', 'images', '10k', 'train')\n",
    "BASE_LABELS_PATH = os.path.join('data', 'bdd100k', 'labels', 'sem_seg', 'masks', 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "[Become one with the data](https://karpathy.github.io/2019/04/25/recipe/#:~:text=1.%20Become%20one%20with%20the%20data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Heatmap\n",
    "\n",
    "https://doc.bdd100k.com/format.html#semantic-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    0: \"road\",\n",
    "    1: \"sidewalk\",\n",
    "    2: \"building\",\n",
    "    3: \"wall\",\n",
    "    4: \"fence\",\n",
    "    5: \"pole\",\n",
    "    6: \"traffic light\",\n",
    "    7: \"traffic sign\",\n",
    "    8: \"vegetation\",\n",
    "    9: \"terrain\",\n",
    "    10: \"sky\",\n",
    "    11: \"person\",\n",
    "    12: \"rider\",\n",
    "    13: \"car\",\n",
    "    14: \"truck\",\n",
    "    15: \"bus\",\n",
    "    16: \"train\",\n",
    "    17: \"motorcycle\",\n",
    "    18: \"bicycle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_folder = BASE_LABELS_PATH\n",
    "\n",
    "target_width, target_height = 128, 228\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "heatmaps = {class_id: np.zeros((target_height, target_width), dtype=np.float32) for class_id in class_dict.keys()}\n",
    "\n",
    "for class_id, class_name in tqdm(class_dict.items(), desc=\"Processing Classes\"):\n",
    "    all_files = [f for f in os.listdir(label_folder) if f.endswith('.png')]\n",
    "    sampled_files = random.sample(all_files, min(N_SAMPLES, len(all_files)))\n",
    "    \n",
    "    for file in tqdm(sampled_files, desc=f\"Sampling {class_name}\", leave=False):\n",
    "        label_path = os.path.join(label_folder, file)\n",
    "        with Image.open(label_path) as img:\n",
    "            label = np.array(img)\n",
    "            \n",
    "            label_resized = np.array(Image.fromarray(label).resize((target_width, target_height), Image.NEAREST))\n",
    "\n",
    "            mask = (label_resized == class_id)\n",
    "            heatmaps[class_id] += mask.astype(np.float32)\n",
    "\n",
    "    heatmaps[class_id] /= len(sampled_files)\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 15))\n",
    "fig.suptitle(\"Spatial Heatmaps for all Classes\", fontsize=20)\n",
    "\n",
    "for class_id, class_name in class_dict.items():\n",
    "    ax = axs[class_id // 5, class_id % 5]\n",
    "    sns.heatmap(heatmaps[class_id], ax=ax, cmap=\"viridis\", cbar=False)\n",
    "    ax.set_title(class_name)\n",
    "    ax.axis('off')\n",
    "\n",
    "for i in range(len(class_dict), 4 * 5):\n",
    "    fig.delaxes(axs[i // 5, i % 5])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Train, Rider, Motorcycle and bicycle seem to be rather underrepresented since these objects' shapes are still clearly visible and don't have a high overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "label_folders = [BASE_LABELS_PATH]\n",
    "num_classes = len(class_dict)\n",
    "class_names = list(class_dict.values())\n",
    "\n",
    "co_occurrence_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
    "\n",
    "for label_folder in label_folders:\n",
    "    all_files = [f for f in os.listdir(label_folder) if f.endswith('.png')]\n",
    "    \n",
    "    for file in tqdm(all_files, desc=f\"Processing Masks in {label_folder}\"):\n",
    "        label_path = os.path.join(label_folder, file)\n",
    "        with Image.open(label_path) as img:\n",
    "            label = np.array(img)\n",
    "\n",
    "            unique_classes = np.unique(label)\n",
    "\n",
    "            for i in range(len(unique_classes)):\n",
    "                for j in range(i, len(unique_classes)):\n",
    "                    class_i = unique_classes[i]\n",
    "                    class_j = unique_classes[j]\n",
    "                    if class_i < num_classes and class_j < num_classes:\n",
    "                        co_occurrence_matrix[class_i, class_j] += 1\n",
    "                        if class_i != class_j:\n",
    "                            co_occurrence_matrix[class_j, class_i] += 1\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(co_occurrence_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Class Co-Occurrence Matrix for Train and Val Sets\", pad=20)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Class\")\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "plt.xticks(rotation=45, ha=\"left\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation Skeleton\n",
    "\n",
    "[Set up the end-to-end training/evaluation skeleton + get dumb baselines](https://karpathy.github.io/2019/04/25/recipe/#:~:text=Set%20up%20the%20end%2Dto%2Dend%20training/evaluation%20skeleton%20%2B%20get%20dumb%20baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split Sizes ---\n",
      "- Train Images: 2518\n",
      "- Val Images: 454\n",
      "- Test Images: 454\n",
      "\n",
      "--- Overlap Report ---\n",
      "✔️ No overlap detected between train and validation sets.\n",
      "✔️ No overlap detected between train and test sets.\n",
      "✔️ No overlap detected between validation and test sets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from data import BDD100KDataset, custom_split_dataset_with_det, check_dataset_overlap\n",
    "\n",
    "DET_TRAIN_PATH = './data/bdd100k/labels/det_20/det_train.json'\n",
    "DET_VAL_PATH = './data/bdd100k/labels/det_20/det_val.json'\n",
    "\n",
    "split_data = custom_split_dataset_with_det(base_data_path=BASE_DATA_PATH, \n",
    "                                           base_labels_path=BASE_LABELS_PATH, \n",
    "                                           det_train_path=DET_TRAIN_PATH, \n",
    "                                           det_val_path=DET_VAL_PATH)\n",
    "\n",
    "check_dataset_overlap(\n",
    "    split_data['train']['image_filenames'],\n",
    "    split_data['val']['image_filenames'],\n",
    "    split_data['test']['image_filenames']\n",
    ")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((72, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((72, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x), dtype=torch.long)),\n",
    "])\n",
    "\n",
    "train_dataset = BDD100KDataset(\n",
    "    images_dir=split_data['train']['data_folder'],\n",
    "    labels_dir=split_data['train']['labels_folder'],\n",
    "    filenames=split_data['train']['image_filenames'],\n",
    "    transform=image_transform,\n",
    "    target_transform=label_transform,\n",
    "    scene_info=split_data['train']['scene_map']\n",
    ")\n",
    "\n",
    "val_dataset = BDD100KDataset(\n",
    "    images_dir=split_data['val']['data_folder'],\n",
    "    labels_dir=split_data['val']['labels_folder'],\n",
    "    filenames=split_data['val']['image_filenames'],\n",
    "    transform=image_transform,\n",
    "    target_transform=label_transform,\n",
    "    scene_info=split_data['val']['scene_map']\n",
    ")\n",
    "\n",
    "test_dataset = BDD100KDataset(\n",
    "    images_dir=split_data['test']['data_folder'],\n",
    "    labels_dir=split_data['test']['labels_folder'],\n",
    "    filenames=split_data['test']['image_filenames'],\n",
    "    transform=image_transform,\n",
    "    target_transform=label_transform,\n",
    "    scene_info=split_data['test']['scene_map']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Train: 100%|██████████| 2518/2518 [00:29<00:00, 84.87it/s]\n",
      "Analyzing Validation: 100%|██████████| 454/454 [00:05<00:00, 81.15it/s]\n",
      "Analyzing Test: 100%|██████████| 454/454 [00:05<00:00, 78.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "def map_class_names_and_order(class_distribution, class_dict):\n",
    "    ordered_classes = sorted(class_dict.keys())  # Ensure consistent class order\n",
    "    class_names = [class_dict[class_id] for class_id in ordered_classes if class_id in class_distribution]\n",
    "    proportions = [class_distribution[class_id] for class_id in ordered_classes if class_id in class_distribution]\n",
    "    return class_names, proportions\n",
    "\n",
    "def plot_class_distribution(class_distribution, title, class_dict):\n",
    "    class_names, proportions = map_class_names_and_order(class_distribution, class_dict)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(class_names, proportions, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    for bar, proportion in zip(bars, proportions):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), \n",
    "                 f\"{proportion * 100:.2f}%\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Proportion of Pixels')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_class_distribution(dataset, num_classes, dataset_name):\n",
    "    class_counts = Counter()\n",
    "    \n",
    "    for idx in trange(len(dataset), desc=f\"Analyzing {dataset_name}\"):\n",
    "        try:\n",
    "            _, mask, _ = dataset[idx]  # Access dataset item\n",
    "            mask_array = np.array(mask)  # Convert mask to numpy array\n",
    "            unique, counts = np.unique(mask_array, return_counts=True)\n",
    "            class_counts.update(dict(zip(unique, counts)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Normalize counts\n",
    "    total_pixels = sum(class_counts.values())\n",
    "    class_distribution = {cls: count / total_pixels for cls, count in class_counts.items()}\n",
    "\n",
    "    return class_counts, class_distribution\n",
    "\n",
    "train_class_counts, train_class_distribution = analyze_class_distribution(train_dataset, num_classes=19, dataset_name=\"Train\")\n",
    "val_class_counts, val_class_distribution = analyze_class_distribution(val_dataset, num_classes=19, dataset_name=\"Validation\")\n",
    "test_class_counts, test_class_distribution = analyze_class_distribution(test_dataset, num_classes=19, dataset_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "plot_class_distribution(train_class_distribution, \"Train Class Distribution\", class_dict)\n",
    "plot_class_distribution(val_class_distribution, \"Validation Class Distribution\", class_dict)\n",
    "plot_class_distribution(test_class_distribution, \"Test Class Distribution\", class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "ordered_class_dists = collections.OrderedDict(sorted(train_class_distribution.items()))\n",
    "class_weights = torch.tensor(list(ordered_class_dists.values()), device=device).float()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: (Tiny-)U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_datalader = DataLoader(train_dataset[:8], batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trainer was already initialized. Skipping wandb initialization.\n",
      "Model unet_baseline already exists! Skipping training.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from core import UNetBaseline\n",
    "\n",
    "model = UNetBaseline(num_classes=19).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255, weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "Trainer(model,\n",
    "        criterion, \n",
    "        optimizer,\n",
    "        epochs=50,\n",
    "        seed=RANDOM_SEED, \n",
    "        device=device, \n",
    "        verbose=True, \n",
    "        run_name=\"unet_baseline\").run(train_dataloader, \n",
    "                                      val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit\n",
    "\n",
    "[Overfit](https://karpathy.github.io/2019/04/25/recipe/#:~:text=3.-,Overfit,-At%20this%20stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_slurm import Slurm\n",
    "\n",
    "slurm = Slurm(\n",
    "    gpus=1,\n",
    "    partition='performance',\n",
    "    time='1-0:0:0',\n",
    "    job_name='job_name',\n",
    "    out=f'slurm_{Slurm.JOB_ID}.log',\n",
    "    error=f'slurm_{Slurm.JOB_ID}.err',\n",
    "    cpus_per_task='16'\n",
    ")\n",
    "\n",
    "slurm.run('python main.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Training with encoder_dims=[64, 128, 256, 512] and decoder_dims=[512, 256, 128, 64]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g0gpkcld) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>train_iou</td><td>█▁</td></tr><tr><td>train_loss</td><td>▁█</td></tr><tr><td>val_iou</td><td>█▁</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_iou</td><td>0.01552</td></tr><tr><td>train_loss</td><td>4.01882</td></tr><tr><td>val_iou</td><td>0.01541</td></tr><tr><td>val_loss</td><td>4.03323</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet_overfit_iteration_1</strong> at: <a href='https://wandb.ai/okaynils/dlbs/runs/g0gpkcld' target=\"_blank\">https://wandb.ai/okaynils/dlbs/runs/g0gpkcld</a><br/> View project at: <a href='https://wandb.ai/okaynils/dlbs' target=\"_blank\">https://wandb.ai/okaynils/dlbs</a><br/>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241204_143744-g0gpkcld\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g0gpkcld). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\wandb\\run-20241204_144103-x8mef9g2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/okaynils/dlbs/runs/x8mef9g2' target=\"_blank\">unet_overfit_iteration_1</a></strong> to <a href='https://wandb.ai/okaynils/dlbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/okaynils/dlbs' target=\"_blank\">https://wandb.ai/okaynils/dlbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/okaynils/dlbs/runs/x8mef9g2' target=\"_blank\">https://wandb.ai/okaynils/dlbs/runs/x8mef9g2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.8648, Train IoU: 0.1291 - Val Loss: 0.5913, Val IoU: 0.1511\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.5913\n",
      "Epoch 2/50 - Train Loss: 0.5270, Train IoU: 0.1554 - Val Loss: 0.5592, Val IoU: 0.1544\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.5592\n",
      "Epoch 3/50 - Train Loss: 0.4710, Train IoU: 0.1635 - Val Loss: 0.5121, Val IoU: 0.1662\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.5121\n",
      "Epoch 4/50 - Train Loss: 0.4407, Train IoU: 0.1685 - Val Loss: 0.4919, Val IoU: 0.1680\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.4919\n",
      "Epoch 5/50 - Train Loss: 0.4138, Train IoU: 0.1730 - Val Loss: 0.4583, Val IoU: 0.1743\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.4583\n",
      "Epoch 6/50 - Train Loss: 0.3936, Train IoU: 0.1766 - Val Loss: 0.4957, Val IoU: 0.1713\n",
      "Epoch 7/50 - Train Loss: 0.3709, Train IoU: 0.1802 - Val Loss: 0.4282, Val IoU: 0.1818\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.4282\n",
      "Epoch 8/50 - Train Loss: 0.3626, Train IoU: 0.1818 - Val Loss: 0.4505, Val IoU: 0.1778\n",
      "Epoch 9/50 - Train Loss: 0.3470, Train IoU: 0.1853 - Val Loss: 0.4600, Val IoU: 0.1754\n",
      "Epoch 10/50 - Train Loss: 0.3397, Train IoU: 0.1869 - Val Loss: 0.4388, Val IoU: 0.1791\n",
      "Epoch 11/50 - Train Loss: 0.3257, Train IoU: 0.1899 - Val Loss: 0.4406, Val IoU: 0.1827\n",
      "Epoch 12/50 - Train Loss: 0.3291, Train IoU: 0.1898 - Val Loss: 0.3996, Val IoU: 0.1848\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.3996\n",
      "Epoch 13/50 - Train Loss: 0.3124, Train IoU: 0.1928 - Val Loss: 0.3888, Val IoU: 0.1890\n",
      "Model saved to models\\unet_overfit_iteration_1_x8mef9g2.pth with val_loss 0.3888\n",
      "Epoch 14/50 - Train Loss: 0.3038, Train IoU: 0.1956 - Val Loss: 0.4113, Val IoU: 0.1906\n",
      "Epoch 15/50 - Train Loss: 0.2886, Train IoU: 0.1987 - Val Loss: 0.4197, Val IoU: 0.1834\n",
      "Epoch 16/50 - Train Loss: 0.2965, Train IoU: 0.1972 - Val Loss: 0.4271, Val IoU: 0.1888\n",
      "Epoch 17/50 - Train Loss: 0.2932, Train IoU: 0.1982 - Val Loss: 0.3959, Val IoU: 0.1897\n",
      "Epoch 18/50 - Train Loss: 0.2738, Train IoU: 0.2022 - Val Loss: 0.4176, Val IoU: 0.1919\n",
      "Epoch 19/50 - Train Loss: 0.2658, Train IoU: 0.2033 - Val Loss: 0.3924, Val IoU: 0.1933\n",
      "Epoch 20/50 - Train Loss: 0.2617, Train IoU: 0.2050 - Val Loss: 0.4125, Val IoU: 0.1904\n",
      "Epoch 21/50 - Train Loss: 0.2566, Train IoU: 0.2056 - Val Loss: 0.4113, Val IoU: 0.1928\n",
      "Epoch 22/50 - Train Loss: 0.2628, Train IoU: 0.2045 - Val Loss: 0.4114, Val IoU: 0.1943\n",
      "Epoch 23/50 - Train Loss: 0.2519, Train IoU: 0.2069 - Val Loss: 0.4217, Val IoU: 0.1894\n",
      "Epoch 24/50 - Train Loss: 0.2334, Train IoU: 0.2113 - Val Loss: 0.4630, Val IoU: 0.1868\n",
      "Epoch 25/50 - Train Loss: 0.2314, Train IoU: 0.2108 - Val Loss: 0.4896, Val IoU: 0.1857\n",
      "Epoch 26/50 - Train Loss: 0.2192, Train IoU: 0.2148 - Val Loss: 0.4207, Val IoU: 0.2027\n",
      "Epoch 27/50 - Train Loss: 0.2033, Train IoU: 0.2184 - Val Loss: 0.4376, Val IoU: 0.1946\n",
      "Epoch 28/50 - Train Loss: 0.2111, Train IoU: 0.2146 - Val Loss: 0.4209, Val IoU: 0.1970\n",
      "Epoch 29/50 - Train Loss: 0.1947, Train IoU: 0.2189 - Val Loss: 0.4051, Val IoU: 0.2034\n",
      "Epoch 30/50 - Train Loss: 0.1861, Train IoU: 0.2220 - Val Loss: 0.4293, Val IoU: 0.2027\n",
      "Epoch 31/50 - Train Loss: 0.1827, Train IoU: 0.2225 - Val Loss: 0.4334, Val IoU: 0.2039\n",
      "Epoch 32/50 - Train Loss: 0.1814, Train IoU: 0.2231 - Val Loss: 0.4226, Val IoU: 0.1993\n",
      "Epoch 33/50 - Train Loss: 0.1717, Train IoU: 0.2265 - Val Loss: 0.4395, Val IoU: 0.1993\n",
      "Epoch 34/50 - Train Loss: 0.1857, Train IoU: 0.2229 - Val Loss: 0.4591, Val IoU: 0.1961\n",
      "Epoch 35/50 - Train Loss: 0.1608, Train IoU: 0.2321 - Val Loss: 0.4335, Val IoU: 0.2053\n",
      "Epoch 36/50 - Train Loss: 0.1495, Train IoU: 0.2379 - Val Loss: 0.4276, Val IoU: 0.2054\n",
      "Epoch 37/50 - Train Loss: 0.1506, Train IoU: 0.2373 - Val Loss: 0.4938, Val IoU: 0.2091\n",
      "Epoch 38/50 - Train Loss: 0.1707, Train IoU: 0.2292 - Val Loss: 0.4558, Val IoU: 0.2054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m\n\u001b[0;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[0;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_SEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m---> 25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_overfit_iteration_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miteration\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m next_dim \u001b[38;5;241m=\u001b[39m encoder_dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     29\u001b[0m encoder_dims\u001b[38;5;241m.\u001b[39mappend(next_dim)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\trainer.py:137\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_weights()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m--> 137\u001b[0m     train_loss, train_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     val_loss, val_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_epoch(val_loader)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\trainer.py:72\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     69\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 72\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\data\\bdd100k_dataset.py:32\u001b[0m, in \u001b[0;36mBDD100KDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 32\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     image \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(image)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\dlbs\\fhnw-ds-dlbs\\.venv\\Lib\\site-packages\\PIL\\Image.py:2365\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2353\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2354\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2355\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2356\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2357\u001b[0m         )\n\u001b[0;32m   2358\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2359\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2360\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2361\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2362\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2363\u001b[0m         )\n\u001b[1;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from core import UNet\n",
    "\n",
    "baseline_encoder_dims = [64, 128, 256, 512]\n",
    "baseline_decoder_dims = [512, 256, 128, 64]\n",
    "\n",
    "encoder_dims = baseline_encoder_dims[:]\n",
    "decoder_dims = baseline_decoder_dims[:]\n",
    "\n",
    "for iteration in range(2):\n",
    "    print(f\"Iteration {iteration + 1}: Training with encoder_dims={encoder_dims} and decoder_dims={decoder_dims}\")\n",
    "    \n",
    "    model = UNet(num_classes=19, encoder_dims=encoder_dims, decoder_dims=decoder_dims).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255, weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    Trainer(model,\n",
    "            criterion, \n",
    "            optimizer,\n",
    "            epochs=50,\n",
    "            seed=RANDOM_SEED, \n",
    "            device=device, \n",
    "            verbose=True, \n",
    "            run_name=f\"unet_overfit_iteration_{iteration + 1}\").run(train_dataloader, \n",
    "                                                                    val_dataloader)\n",
    "    \n",
    "    next_dim = encoder_dims[-1] * 2\n",
    "    encoder_dims.append(next_dim)\n",
    "    decoder_dims.insert(0, next_dim)\n",
    "    print(decoder_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "[Regularize](https://karpathy.github.io/2019/04/25/recipe/#:~:text=4.-,Regularize,-Ideally%2C%20we%20are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the model\n",
    "\n",
    "[Tune](https://karpathy.github.io/2019/04/25/recipe/#:~:text=5.-,Tune,-You%20should%20now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles & Leave it training\n",
    "\n",
    "[Squeeze out the juice](https://karpathy.github.io/2019/04/25/recipe/#:~:text=Squeeze%20out%20the%20juice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
