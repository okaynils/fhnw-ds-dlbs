@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2019},
  month = apr,
  urldate = {2024-12-03},
  howpublished = {https://karpathy.github.io/2019/04/25/recipe/},
  file = {/Users/nils/Zotero/storage/96Z2XFRB/recipe.html}
}

@misc{oktayAttentionUNetLearning2018,
  title = {Attention {{U-Net}}: {{Learning Where}} to {{Look}} for the {{Pancreas}}},
  shorttitle = {Attention {{U-Net}}},
  author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  year = {2018},
  month = may,
  number = {arXiv:1804.03999},
  eprint = {1804.03999},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.03999},
  urldate = {2024-12-03},
  abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nils/Zotero/storage/R65WEL5C/Oktay et al. - 2018 - Attention U-Net Learning Where to Look for the Pa.pdf;/Users/nils/Zotero/storage/5HZUDZ5K/1804.html}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2024-12-03},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nils/Zotero/storage/XKCZY7X3/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/Users/nils/Zotero/storage/8HHJ6K6P/1505.html}
}

@misc{yuBDD100KDiverseDriving2020,
  title = {{{BDD100K}}: {{A Diverse Driving Dataset}} for {{Heterogeneous Multitask Learning}}},
  shorttitle = {{{BDD100K}}},
  author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  year = {2020},
  month = apr,
  number = {arXiv:1805.04687},
  eprint = {1805.04687},
  publisher = {arXiv},
  urldate = {2024-11-25},
  abstract = {Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nils/Zotero/storage/PELZMNLF/Yu et al. - 2020 - BDD100K A Diverse Driving Dataset for Heterogeneo.pdf;/Users/nils/Zotero/storage/2U38JF9H/1805.html}
}
